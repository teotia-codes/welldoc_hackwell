{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RiskEngine End-to-End (30–180d history → 90d risk)\n",
        "This notebook builds a risk prediction pipeline over your four CSVs.\n",
        "\n",
        "**Inputs expected:**\n",
        "- `Patient_info.csv`\n",
        "- `Biochemical_parameters.csv`\n",
        "- `Diagnostics.csv`\n",
        "- `Glucose_measurements.csv` (large; we'll aggregate to daily metrics)\n",
        "\n",
        "**Outputs:**\n",
        "- `outputs/feature_dataset.parquet` – modeling dataset with rolling-window features\n",
        "- `outputs/model_calibrated.joblib` – trained + calibrated model\n",
        "- `outputs/metrics.json` – AUROC, AUPRC, Brier, confusion matrix\n",
        "- `outputs/plots/` – ROC, PR, Calibration plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using XGBoost: True | SHAP available: True\n"
          ]
        }
      ],
      "source": [
        "# --- Setup ---\n",
        "import os, json, math, gc, sys, warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, confusion_matrix\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prefer xgboost; fallback to HistGradientBoosting if not available\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "except Exception:\n",
        "    HAS_SHAP = False\n",
        "\n",
        "BASE = Path(\".\").resolve()\n",
        "OUT = BASE / \"outputs\"\n",
        "PLOTS = OUT / \"plots\"\n",
        "for p in [OUT, PLOTS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Using XGBoost:\", HAS_XGB, \"| SHAP available:\", HAS_SHAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Config: set your file paths here ---\n",
        "PATH_PATIENT_INFO = \"Patient_info.csv\"\n",
        "PATH_LABS = \"Biochemical_parameters.csv\"\n",
        "PATH_DIAG = \"Diagnostics.csv\"\n",
        "PATH_CGM = \"Glucose_measurements.csv\"   # large file\n",
        "\n",
        "# CGM cadence assumption (15-min ~ 96/day)\n",
        "EXPECTED_READS_PER_DAY = 96\n",
        "\n",
        "# Rolling windows (in days)\n",
        "WINDOWS = [30, 90, 180]\n",
        "\n",
        "# Label thresholds\n",
        "FUTURE_90D_HYPER_MEAN_THRESHOLD = 0.35   # avg future 90d fraction > 180 mg/dL\n",
        "A1C_RISE_THRESHOLD = 0.5                 # A1c increase >= 0.5 absolute within 90d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patients: 736 | Labs coverage: 723 | Diagnostics coverage: 511\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(  Patient_ID Reception_date                              Name  Value\n",
              " 0  LIB193265     2018-05-09                         Potassium    4.5\n",
              " 1  LIB193265     2018-05-09                   HDL cholesterol   64.0\n",
              " 2  LIB193265     2018-05-09  Gamma-glutamyl Transferase (GGT)   11.0,\n",
              "   Patient_ID    Code                                        Description\n",
              " 0  LIB193263   272.4               Other and unspecified hyperlipidemia\n",
              " 1  LIB193264   354.0                             Carpal tunnel syndrome\n",
              " 2  LIB193264  574.00  Calculus of gallbladder with acute cholecystit...,\n",
              "   Patient_ID Sex  Birth_year Initial_measurement_date Final_measurement_date  \\\n",
              " 0  LIB193263   M        1965               2020-09-06             2022-03-19   \n",
              " 1  LIB193264   F        1975               2020-10-06             2022-03-19   \n",
              " 2  LIB193265   F        1980                      NaT             2022-03-19   \n",
              " \n",
              "    Number_of_days_with_measures  Number_of_measurements  \\\n",
              " 0                           648                   60097   \n",
              " 1                           326                   26786   \n",
              " 2                           581                   46575   \n",
              " \n",
              "   Initial_biochemical_parameters_date Final_biochemical_parameters_date  \\\n",
              " 0                                 NaT                               NaT   \n",
              " 1                                 NaT                               NaT   \n",
              " 2                          2018-05-09                        2021-01-10   \n",
              " \n",
              "    Number_of_biochemical_parameters  Number_of_diagnostics  \n",
              " 0                               NaN                    1.0  \n",
              " 1                               NaN                    3.0  \n",
              " 2                             120.0                    NaN  )"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Load the three small tables ---\n",
        "pi = pd.read_csv(PATH_PATIENT_INFO)\n",
        "bp = pd.read_csv(PATH_LABS)\n",
        "dg = pd.read_csv(PATH_DIAG)\n",
        "\n",
        "# Parse dates in patient_info\n",
        "for c in [\"Initial_measurement_date\",\"Final_measurement_date\",\n",
        "          \"Initial_biochemical_parameters_date\",\"Final_biochemical_parameters_date\"]:\n",
        "    if c in pi.columns:\n",
        "        pi[c] = pd.to_datetime(pi[c], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "# Parse labs\n",
        "if \"Reception_date\" in bp.columns:\n",
        "    bp[\"Reception_date\"] = pd.to_datetime(bp[\"Reception_date\"], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "print(\"Patients:\", pi['Patient_ID'].nunique(), \n",
        "      \"| Labs coverage:\", bp['Patient_ID'].nunique(), \n",
        "      \"| Diagnostics coverage:\", dg['Patient_ID'].nunique())\n",
        "bp.head(3), dg.head(3), pi.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_cgm_daily(path_cgm, expected_reads_per_day=96, chunksize=2_000_000):\n",
        "    import pandas as pd, numpy as np, math, gc\n",
        "\n",
        "    required_cols = [\"Patient_ID\",\"Measurement_date\",\"Measurement_time\",\"Measurement\"]\n",
        "    usecols = required_cols  # restrict to just the needed columns\n",
        "    dtypes  = {\"Patient_ID\":\"string\", \"Measurement\":\"float32\"}\n",
        "\n",
        "    agg = {}\n",
        "\n",
        "    for chunk in pd.read_csv(\n",
        "        path_cgm,\n",
        "        usecols=usecols,\n",
        "        dtype=dtypes,\n",
        "        chunksize=chunksize,\n",
        "        low_memory=True,\n",
        "        on_bad_lines=\"skip\",           # skip malformed lines\n",
        "        encoding_errors=\"replace\",     # be forgiving on encoding\n",
        "        na_values=[\"\", \"NA\", \"NaN\", \"null\", \"None\"]\n",
        "    ):\n",
        "        # normalize column names (in case of stray whitespace)\n",
        "        chunk.columns = [c.strip() for c in chunk.columns]\n",
        "\n",
        "        # coerce date; keep only valid rows with a numeric measurement and patient id\n",
        "        chunk[\"Measurement_date\"] = pd.to_datetime(\n",
        "            chunk[\"Measurement_date\"], dayfirst=True, errors=\"coerce\"\n",
        "        ).dt.date\n",
        "\n",
        "        chunk = chunk.dropna(subset=[\"Patient_ID\",\"Measurement_date\",\"Measurement\"])\n",
        "        if chunk.empty:\n",
        "            continue\n",
        "\n",
        "        # flags\n",
        "        chunk[\"gt_180\"] = (chunk[\"Measurement\"] > 180).astype(\"int16\")\n",
        "        chunk[\"lt_70\"]  = (chunk[\"Measurement\"] < 70).astype(\"int16\")\n",
        "\n",
        "        # group by patient/day\n",
        "        grp = (chunk\n",
        "               .groupby([\"Patient_ID\",\"Measurement_date\"], as_index=False)\n",
        "               .agg(n_reads=(\"Measurement\",\"size\"),\n",
        "                    mean_glu=(\"Measurement\",\"mean\"),\n",
        "                    std_glu=(\"Measurement\",\"std\"),\n",
        "                    max_glu=(\"Measurement\",\"max\"),\n",
        "                    min_glu=(\"Measurement\",\"min\"),\n",
        "                    gt_180_sum=(\"gt_180\",\"sum\"),\n",
        "                    lt_70_sum=(\"lt_70\",\"sum\"))\n",
        "              )\n",
        "\n",
        "        # ignore any accidental zero-read groups (shouldn't exist, but safe)\n",
        "        grp = grp[grp[\"n_reads\"] > 0]\n",
        "        if grp.empty:\n",
        "            continue\n",
        "\n",
        "        # accumulate\n",
        "        for _, r in grp.iterrows():\n",
        "            key = (r[\"Patient_ID\"], r[\"Measurement_date\"])\n",
        "            n   = int(r[\"n_reads\"])\n",
        "            mu  = float(r[\"mean_glu\"])\n",
        "            sd  = float(0.0 if pd.isna(r[\"std_glu\"]) else r[\"std_glu\"])\n",
        "\n",
        "            if key not in agg:\n",
        "                agg[key] = {\n",
        "                    \"Patient_ID\": r[\"Patient_ID\"],\n",
        "                    \"date\": pd.to_datetime(str(r[\"Measurement_date\"])),\n",
        "                    \"n_reads\": n,\n",
        "                    \"sum_glu\": mu * n,\n",
        "                    \"sum_sq_glu\": (sd*sd + mu*mu) * n,\n",
        "                    \"max_glu\": float(r[\"max_glu\"]),\n",
        "                    \"min_glu\": float(r[\"min_glu\"]),\n",
        "                    \"gt_180\": int(r[\"gt_180_sum\"]),\n",
        "                    \"lt_70\": int(r[\"lt_70_sum\"]),\n",
        "                }\n",
        "            else:\n",
        "                a = agg[key]\n",
        "                a[\"n_reads\"] += n\n",
        "                a[\"sum_glu\"] += mu * n\n",
        "                a[\"sum_sq_glu\"] += (sd*sd + mu*mu) * n\n",
        "                a[\"max_glu\"] = max(a[\"max_glu\"], float(r[\"max_glu\"]))\n",
        "                a[\"min_glu\"] = min(a[\"min_glu\"], float(r[\"min_glu\"]))\n",
        "                a[\"gt_180\"]  += int(r[\"gt_180_sum\"])\n",
        "                a[\"lt_70\"]   += int(r[\"lt_70_sum\"])\n",
        "\n",
        "        del chunk, grp\n",
        "        gc.collect()\n",
        "\n",
        "    # materialize\n",
        "    rows = []\n",
        "    for (pid, day), a in agg.items():\n",
        "        n = max(1, a[\"n_reads\"])\n",
        "        mean = a[\"sum_glu\"] / n\n",
        "        var  = max(0.0, a[\"sum_sq_glu\"]/n - mean*mean)\n",
        "        std  = math.sqrt(var)\n",
        "        rows.append({\n",
        "            \"Patient_ID\": pid,\n",
        "            \"date\": a[\"date\"],\n",
        "            \"n_reads\": a[\"n_reads\"],\n",
        "            \"coverage\": min(1.0, a[\"n_reads\"] / expected_reads_per_day),\n",
        "            \"mean_glu\": mean,\n",
        "            \"std_glu\": std,\n",
        "            \"max_glu\": a[\"max_glu\"],\n",
        "            \"min_glu\": a[\"min_glu\"],\n",
        "            \"frac_gt_180\": a[\"gt_180\"] / n,\n",
        "            \"frac_lt_70\": a[\"lt_70\"] / n,\n",
        "            \"tir_70_180\": max(0.0, 1.0 - (a[\"gt_180\"] + a[\"lt_70\"]) / n),\n",
        "        })\n",
        "\n",
        "    daily = pd.DataFrame(rows).sort_values([\"Patient_ID\",\"date\"]).reset_index(drop=True)\n",
        "    # enforce basic quality filters here so no 0-read rows survive\n",
        "    daily = daily[(daily[\"n_reads\"] > 0)]\n",
        "    return daily\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41abcc49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(184269, 11)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>date</th>\n",
              "      <th>n_reads</th>\n",
              "      <th>coverage</th>\n",
              "      <th>mean_glu</th>\n",
              "      <th>std_glu</th>\n",
              "      <th>max_glu</th>\n",
              "      <th>min_glu</th>\n",
              "      <th>frac_gt_180</th>\n",
              "      <th>frac_lt_70</th>\n",
              "      <th>tir_70_180</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>90</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>135.677780</td>\n",
              "      <td>41.512100</td>\n",
              "      <td>244.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-08</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>173.541672</td>\n",
              "      <td>61.597149</td>\n",
              "      <td>356.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.322917</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.656250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-09</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>214.708328</td>\n",
              "      <td>63.121422</td>\n",
              "      <td>331.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-10</td>\n",
              "      <td>92</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>149.858688</td>\n",
              "      <td>35.584778</td>\n",
              "      <td>242.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.826087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-11</td>\n",
              "      <td>92</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>130.076080</td>\n",
              "      <td>38.457409</td>\n",
              "      <td>211.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-12</td>\n",
              "      <td>94</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>172.765961</td>\n",
              "      <td>111.982666</td>\n",
              "      <td>382.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.436170</td>\n",
              "      <td>0.223404</td>\n",
              "      <td>0.340426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-07</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>168.666672</td>\n",
              "      <td>48.101574</td>\n",
              "      <td>262.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.052083</td>\n",
              "      <td>0.531250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-08</td>\n",
              "      <td>81</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>157.629623</td>\n",
              "      <td>35.000160</td>\n",
              "      <td>248.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>0.209877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.790123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>94</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>212.308517</td>\n",
              "      <td>24.608778</td>\n",
              "      <td>257.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-10</td>\n",
              "      <td>91</td>\n",
              "      <td>0.947917</td>\n",
              "      <td>169.208786</td>\n",
              "      <td>53.145191</td>\n",
              "      <td>289.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.274725</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.670330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID       date  n_reads  coverage    mean_glu     std_glu  max_glu  \\\n",
              "0  LIB193263 2020-01-07       90  0.937500  135.677780   41.512100    244.0   \n",
              "1  LIB193263 2020-01-08       96  1.000000  173.541672   61.597149    356.0   \n",
              "2  LIB193263 2020-01-09       96  1.000000  214.708328   63.121422    331.0   \n",
              "3  LIB193263 2020-01-10       92  0.958333  149.858688   35.584778    242.0   \n",
              "4  LIB193263 2020-01-11       92  0.958333  130.076080   38.457409    211.0   \n",
              "5  LIB193263 2020-01-12       94  0.979167  172.765961  111.982666    382.0   \n",
              "6  LIB193263 2020-02-07       96  1.000000  168.666672   48.101574    262.0   \n",
              "7  LIB193263 2020-02-08       81  0.843750  157.629623   35.000160    248.0   \n",
              "8  LIB193263 2020-02-09       94  0.979167  212.308517   24.608778    257.0   \n",
              "9  LIB193263 2020-02-10       91  0.947917  169.208786   53.145191    289.0   \n",
              "\n",
              "   min_glu  frac_gt_180  frac_lt_70  tir_70_180  \n",
              "0     77.0     0.133333    0.000000    0.866667  \n",
              "1     64.0     0.322917    0.020833    0.656250  \n",
              "2     98.0     0.666667    0.000000    0.333333  \n",
              "3     80.0     0.173913    0.000000    0.826087  \n",
              "4     82.0     0.130435    0.000000    0.869565  \n",
              "5     43.0     0.436170    0.223404    0.340426  \n",
              "6     55.0     0.416667    0.052083    0.531250  \n",
              "7     99.0     0.209877    0.000000    0.790123  \n",
              "8    164.0     0.829787    0.000000    0.170213  \n",
              "9     57.0     0.274725    0.054945    0.670330  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "daily_cgm = aggregate_cgm_daily(PATH_CGM, expected_reads_per_day=EXPECTED_READS_PER_DAY)\n",
        "daily_cgm.to_parquet(OUT / \"daily_cgm.parquet\", index=False)\n",
        "print(daily_cgm.shape)\n",
        "daily_cgm.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded daily CGM: (184269, 11)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>date</th>\n",
              "      <th>n_reads</th>\n",
              "      <th>coverage</th>\n",
              "      <th>mean_glu</th>\n",
              "      <th>std_glu</th>\n",
              "      <th>max_glu</th>\n",
              "      <th>min_glu</th>\n",
              "      <th>frac_gt_180</th>\n",
              "      <th>frac_lt_70</th>\n",
              "      <th>tir_70_180</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>90</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>135.677780</td>\n",
              "      <td>41.512100</td>\n",
              "      <td>244.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-08</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>173.541672</td>\n",
              "      <td>61.597149</td>\n",
              "      <td>356.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.322917</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.656250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-09</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>214.708328</td>\n",
              "      <td>63.121422</td>\n",
              "      <td>331.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-10</td>\n",
              "      <td>92</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>149.858688</td>\n",
              "      <td>35.584778</td>\n",
              "      <td>242.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.826087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-01-11</td>\n",
              "      <td>92</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>130.076080</td>\n",
              "      <td>38.457409</td>\n",
              "      <td>211.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID       date  n_reads  coverage    mean_glu    std_glu  max_glu  \\\n",
              "0  LIB193263 2020-01-07       90  0.937500  135.677780  41.512100    244.0   \n",
              "1  LIB193263 2020-01-08       96  1.000000  173.541672  61.597149    356.0   \n",
              "2  LIB193263 2020-01-09       96  1.000000  214.708328  63.121422    331.0   \n",
              "3  LIB193263 2020-01-10       92  0.958333  149.858688  35.584778    242.0   \n",
              "4  LIB193263 2020-01-11       92  0.958333  130.076080  38.457409    211.0   \n",
              "\n",
              "   min_glu  frac_gt_180  frac_lt_70  tir_70_180  \n",
              "0     77.0     0.133333    0.000000    0.866667  \n",
              "1     64.0     0.322917    0.020833    0.656250  \n",
              "2     98.0     0.666667    0.000000    0.333333  \n",
              "3     80.0     0.173913    0.000000    0.826087  \n",
              "4     82.0     0.130435    0.000000    0.869565  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Quick load of pre-aggregated daily CGM if you've already generated it ---\n",
        "daily_path = OUT / \"daily_cgm.parquet\"\n",
        "if daily_path.exists():\n",
        "    daily_cgm = pd.read_parquet(daily_path)\n",
        "    print(\"Loaded daily CGM:\", daily_cgm.shape)\n",
        "    display(daily_cgm.head())\n",
        "else:\n",
        "    print(\"Daily CGM parquet not found. Generate it in the previous cell when you have the big file locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1c rows: (1963, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>date</th>\n",
              "      <th>a1c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2018-05-09</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2019-12-03</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2020-02-06</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>2021-05-04</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>2021-05-11</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Patient_ID       date  a1c\n",
              "11   LIB193265 2018-05-09  8.6\n",
              "33   LIB193265 2019-12-03  7.5\n",
              "60   LIB193265 2020-02-06  7.3\n",
              "165  LIB193266 2021-05-04  7.5\n",
              "169  LIB193266 2021-05-11  7.5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Prepare A1c time series per patient (for features + label proxy) ---\n",
        "# We filter labs to A1c and keep (Patient_ID, date, value)\n",
        "a1c = bp[bp[\"Name\"].str.contains(\"Glycated hemoglobin\", case=False, na=False)].copy()\n",
        "a1c = a1c.rename(columns={\"Reception_date\":\"date\",\"Value\":\"a1c\"})\n",
        "a1c = a1c[[\"Patient_ID\",\"date\",\"a1c\"]].dropna()\n",
        "a1c = a1c.sort_values([\"Patient_ID\",\"date\"])\n",
        "print(\"A1c rows:\", a1c.shape)\n",
        "display(a1c.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diag_flags shape: (297, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>diag_244.9</th>\n",
              "      <th>diag_250.4</th>\n",
              "      <th>diag_250.5</th>\n",
              "      <th>diag_250.51</th>\n",
              "      <th>diag_250.6</th>\n",
              "      <th>diag_272.0</th>\n",
              "      <th>diag_272.4</th>\n",
              "      <th>diag_278.00</th>\n",
              "      <th>diag_354.0</th>\n",
              "      <th>diag_362.01</th>\n",
              "      <th>diag_401.9</th>\n",
              "      <th>diag_477.0</th>\n",
              "      <th>diag_477.9</th>\n",
              "      <th>diag_493.9</th>\n",
              "      <th>diag_733.00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LIB193264</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LIB193272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LIB193273</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIB193274</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID  diag_244.9  diag_250.4  diag_250.5  diag_250.51  diag_250.6  \\\n",
              "0  LIB193263           0           0           0            0           0   \n",
              "1  LIB193264           0           0           0            0           0   \n",
              "2  LIB193272           0           0           0            1           0   \n",
              "3  LIB193273           0           0           0            0           0   \n",
              "4  LIB193274           0           0           1            0           0   \n",
              "\n",
              "   diag_272.0  diag_272.4  diag_278.00  diag_354.0  diag_362.01  diag_401.9  \\\n",
              "0           0           1            0           0            0           0   \n",
              "1           0           0            0           1            0           0   \n",
              "2           0           0            0           0            0           0   \n",
              "3           0           0            0           0            0           0   \n",
              "4           0           0            0           1            0           0   \n",
              "\n",
              "   diag_477.0  diag_477.9  diag_493.9  diag_733.00  \n",
              "0           0           0           0            0  \n",
              "1           0           0           0            0  \n",
              "2           0           0           0            0  \n",
              "3           1           0           1            0  \n",
              "4           0           0           0            0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# dg is your Diagnostics.csv as a DataFrame\n",
        "# expect columns like: Patient_ID, Code (ICD), maybe others\n",
        "top_codes = dg[\"Code\"].value_counts().head(15).index.tolist()\n",
        "\n",
        "diag_flags = (\n",
        "    dg.loc[dg[\"Code\"].isin(top_codes), [\"Patient_ID\",\"Code\"]]\n",
        "      .assign(val=1)\n",
        "      .pivot_table(index=\"Patient_ID\", columns=\"Code\", values=\"val\", aggfunc=\"max\", fill_value=0)\n",
        ")\n",
        "\n",
        "# prefix and tidy\n",
        "diag_flags.columns = [f\"diag_{c}\" for c in diag_flags.columns]\n",
        "diag_flags = diag_flags.reset_index()\n",
        "\n",
        "print(\"diag_flags shape:\", diag_flags.shape)\n",
        "display(diag_flags.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e0cc7d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diag_flags (padded) shape: (734, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>diag_244.9</th>\n",
              "      <th>diag_250.4</th>\n",
              "      <th>diag_250.5</th>\n",
              "      <th>diag_250.51</th>\n",
              "      <th>diag_250.6</th>\n",
              "      <th>diag_272.0</th>\n",
              "      <th>diag_272.4</th>\n",
              "      <th>diag_278.00</th>\n",
              "      <th>diag_354.0</th>\n",
              "      <th>diag_362.01</th>\n",
              "      <th>diag_401.9</th>\n",
              "      <th>diag_477.0</th>\n",
              "      <th>diag_477.9</th>\n",
              "      <th>diag_493.9</th>\n",
              "      <th>diag_733.00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LIB193264</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIB193267</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID  diag_244.9  diag_250.4  diag_250.5  diag_250.51  diag_250.6  \\\n",
              "0  LIB193263           0           0           0            0           0   \n",
              "1  LIB193264           0           0           0            0           0   \n",
              "2  LIB193265           0           0           0            0           0   \n",
              "3  LIB193266           0           0           0            0           0   \n",
              "4  LIB193267           0           0           0            0           0   \n",
              "\n",
              "   diag_272.0  diag_272.4  diag_278.00  diag_354.0  diag_362.01  diag_401.9  \\\n",
              "0           0           1            0           0            0           0   \n",
              "1           0           0            0           1            0           0   \n",
              "2           0           0            0           0            0           0   \n",
              "3           0           0            0           0            0           0   \n",
              "4           0           0            0           0            0           0   \n",
              "\n",
              "   diag_477.0  diag_477.9  diag_493.9  diag_733.00  \n",
              "0           0           0           0            0  \n",
              "1           0           0           0            0  \n",
              "2           0           0           0            0  \n",
              "3           0           0           0            0  \n",
              "4           0           0           0            0  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure every Patient_ID in daily_cgm appears in diag_flags with 0s\n",
        "all_pids = pd.DataFrame({\"Patient_ID\": daily_cgm[\"Patient_ID\"].unique()})\n",
        "diag_cols = [c for c in diag_flags.columns if c.startswith(\"diag_\")]\n",
        "\n",
        "# left-join and fill missing diag_* with 0\n",
        "diag_flags = all_pids.merge(diag_flags, on=\"Patient_ID\", how=\"left\")\n",
        "for c in diag_cols:\n",
        "    if c in diag_flags.columns:\n",
        "        diag_flags[c] = diag_flags[c].fillna(0).astype(int)\n",
        "    else:\n",
        "        # if a top code column is completely missing, create it\n",
        "        diag_flags[c] = 0\n",
        "\n",
        "# keep only Patient_ID + diag_* columns\n",
        "keep_cols = [\"Patient_ID\"] + diag_cols\n",
        "diag_flags = diag_flags[keep_cols].copy()\n",
        "\n",
        "print(\"diag_flags (padded) shape:\", diag_flags.shape)\n",
        "diag_flags.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "996723bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEFT daily_feat (for asof):\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'daily_feat' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLEFT daily_feat (for asof):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdaily_feat\u001b[49m[[\u001b[33m\"\u001b[39m\u001b[33mPatient_ID\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mindex_date\u001b[39m\u001b[33m\"\u001b[39m]].head(\u001b[32m15\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(daily_feat[[\u001b[33m\"\u001b[39m\u001b[33mPatient_ID\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mindex_date\u001b[39m\u001b[33m\"\u001b[39m]].tail(\u001b[32m15\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdtype:\u001b[39m\u001b[33m\"\u001b[39m, daily_feat[\u001b[33m\"\u001b[39m\u001b[33mindex_date\u001b[39m\u001b[33m\"\u001b[39m].dtype)\n",
            "\u001b[31mNameError\u001b[39m: name 'daily_feat' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"LEFT daily_feat (for asof):\")\n",
        "print(daily_feat[[\"Patient_ID\",\"index_date\"]].head(15))\n",
        "print(daily_feat[[\"Patient_ID\",\"index_date\"]].tail(15))\n",
        "print(\"dtype:\", daily_feat[\"index_date\"].dtype)\n",
        "\n",
        "print(\"\\nRIGHT a1c_prior_tbl:\")\n",
        "print(a1c_prior_tbl[[\"Patient_ID\",\"a1c_date\"]].head(15))\n",
        "print(a1c_prior_tbl[[\"Patient_ID\",\"a1c_date\"]].tail(15))\n",
        "print(\"dtype:\", a1c_prior_tbl[\"a1c_date\"].dtype)\n",
        "\n",
        "# extra check: monotonicity within each Patient_ID\n",
        "print(\"\\nNon-monotonic counts LEFT:\",\n",
        "      (daily_feat.groupby(\"Patient_ID\")[\"index_date\"].diff() < pd.Timedelta(0)).sum())\n",
        "print(\"Non-monotonic counts RIGHT:\",\n",
        "      (a1c_prior_tbl.groupby(\"Patient_ID\")[\"a1c_date\"].diff() < pd.Timedelta(0)).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modeling dataset shape: (174812, 70)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>date</th>\n",
              "      <th>index_date</th>\n",
              "      <th>label_90d</th>\n",
              "      <th>n_reads</th>\n",
              "      <th>coverage</th>\n",
              "      <th>mean_glu_mean_30</th>\n",
              "      <th>mean_glu_std_30</th>\n",
              "      <th>std_glu_mean_30</th>\n",
              "      <th>std_glu_std_30</th>\n",
              "      <th>...</th>\n",
              "      <th>diag_278.00</th>\n",
              "      <th>diag_354.0</th>\n",
              "      <th>diag_362.01</th>\n",
              "      <th>diag_401.9</th>\n",
              "      <th>diag_477.0</th>\n",
              "      <th>diag_477.9</th>\n",
              "      <th>diag_493.9</th>\n",
              "      <th>diag_733.00</th>\n",
              "      <th>a1c_prior</th>\n",
              "      <th>a1c_future</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-07</td>\n",
              "      <td>2020-02-08</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>163.613597</td>\n",
              "      <td>28.615002</td>\n",
              "      <td>57.193871</td>\n",
              "      <td>26.466748</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-08</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>1</td>\n",
              "      <td>81</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>162.865601</td>\n",
              "      <td>26.576687</td>\n",
              "      <td>54.419657</td>\n",
              "      <td>25.729147</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>2020-02-10</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>168.359258</td>\n",
              "      <td>29.827046</td>\n",
              "      <td>51.107337</td>\n",
              "      <td>26.038118</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID       date index_date  label_90d  n_reads  coverage  \\\n",
              "6  LIB193263 2020-02-07 2020-02-08          1       96  1.000000   \n",
              "7  LIB193263 2020-02-08 2020-02-09          1       81  0.843750   \n",
              "8  LIB193263 2020-02-09 2020-02-10          1       94  0.979167   \n",
              "\n",
              "   mean_glu_mean_30  mean_glu_std_30  std_glu_mean_30  std_glu_std_30  ...  \\\n",
              "6        163.613597        28.615002        57.193871       26.466748  ...   \n",
              "7        162.865601        26.576687        54.419657       25.729147  ...   \n",
              "8        168.359258        29.827046        51.107337       26.038118  ...   \n",
              "\n",
              "   diag_278.00  diag_354.0  diag_362.01  diag_401.9  diag_477.0  diag_477.9  \\\n",
              "6            0           0            0           0           0           0   \n",
              "7            0           0            0           0           0           0   \n",
              "8            0           0            0           0           0           0   \n",
              "\n",
              "   diag_493.9  diag_733.00  a1c_prior  a1c_future  \n",
              "6           0            0        NaN         NaN  \n",
              "7           0            0        NaN         NaN  \n",
              "8           0            0        NaN         NaN  \n",
              "\n",
              "[3 rows x 70 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ================================\n",
        "# Build rolling-window features + labels (no-merge_asof version)\n",
        "# ================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ---------- Guards ----------\n",
        "if \"daily_cgm\" not in globals():\n",
        "    raise RuntimeError(\"daily_cgm not loaded. Create or load outputs/daily_cgm.parquet first.\")\n",
        "if \"pi\" not in globals():\n",
        "    raise RuntimeError(\"pi (Patient_info) not loaded.\")\n",
        "if \"a1c\" not in globals():\n",
        "    a1c = pd.DataFrame(columns=[\"Patient_ID\",\"date\",\"a1c\"])\n",
        "if \"diag_flags\" not in globals():\n",
        "    diag_flags = pd.DataFrame(columns=[\"Patient_ID\"])\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def _ensure_dt_naive(dt_series: pd.Series) -> pd.Series:\n",
        "    s = pd.to_datetime(dt_series, errors=\"coerce\")\n",
        "    if getattr(s.dtype, \"tz\", None) is not None:\n",
        "        s = s.dt.tz_convert(None)\n",
        "    return s\n",
        "\n",
        "# ---------- Prep inputs ----------\n",
        "daily_cgm = daily_cgm.copy()\n",
        "daily_cgm[\"Patient_ID\"] = daily_cgm[\"Patient_ID\"].astype(\"string\")\n",
        "daily_cgm[\"date\"]       = _ensure_dt_naive(daily_cgm[\"date\"])\n",
        "\n",
        "a1c = a1c.copy()\n",
        "if len(a1c):\n",
        "    a1c[\"Patient_ID\"] = a1c[\"Patient_ID\"].astype(\"string\")\n",
        "    a1c[\"date\"]       = _ensure_dt_naive(a1c[\"date\"])\n",
        "    a1c = a1c.dropna(subset=[\"Patient_ID\",\"date\",\"a1c\"]).sort_values([\"Patient_ID\",\"date\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- Rolling features ----------\n",
        "def add_rolling_features(df, cols, windows):\n",
        "    df = df.sort_values([\"Patient_ID\",\"date\"]).copy()\n",
        "    g = df.groupby(\"Patient_ID\", sort=False)\n",
        "    for w in windows:\n",
        "        for c in cols:\n",
        "            df[f\"{c}_mean_{w}\"] = g[c].transform(lambda s: s.rolling(window=w, min_periods=7).mean())\n",
        "            df[f\"{c}_std_{w}\"]  = g[c].transform(lambda s: s.rolling(window=w, min_periods=7).std())\n",
        "        def slope_rolling(s):\n",
        "            out = np.full(len(s), np.nan, dtype=\"float64\")\n",
        "            for i in range(len(s)):\n",
        "                start = max(0, i - w + 1)\n",
        "                window = s.iloc[start:i+1].dropna()\n",
        "                if len(window) >= 7:\n",
        "                    t = np.arange(len(window)).reshape(-1,1)\n",
        "                    lr = LinearRegression().fit(t, window.values)\n",
        "                    out[i] = float(lr.coef_[0])\n",
        "            return pd.Series(out, index=s.index)\n",
        "        df[f\"mean_glu_slope_{w}\"] = g[\"mean_glu\"].transform(slope_rolling)\n",
        "    return df\n",
        "\n",
        "feat_cols = [\"mean_glu\",\"std_glu\",\"tir_70_180\",\"frac_gt_180\",\"frac_lt_70\",\"coverage\",\"n_reads\"]\n",
        "daily_feat = add_rolling_features(daily_cgm, feat_cols, WINDOWS)\n",
        "\n",
        "# ---------- Index date & demographics ----------\n",
        "daily_feat[\"index_date\"] = _ensure_dt_naive(daily_feat[\"date\"]) + pd.Timedelta(days=1)\n",
        "pi_dem = pi[[\"Patient_ID\",\"Sex\",\"Birth_year\"]].copy()\n",
        "pi_dem[\"Patient_ID\"] = pi_dem[\"Patient_ID\"].astype(\"string\")\n",
        "daily_feat = daily_feat.merge(pi_dem, on=\"Patient_ID\", how=\"left\")\n",
        "\n",
        "# ---------- Diagnostics ----------\n",
        "diag_flags = diag_flags.copy()\n",
        "diag_flags[\"Patient_ID\"] = diag_flags[\"Patient_ID\"].astype(\"string\")\n",
        "daily_feat = daily_feat.merge(diag_flags, on=\"Patient_ID\", how=\"left\")\n",
        "diag_cols = [c for c in daily_feat.columns if c.startswith(\"diag_\")]\n",
        "if diag_cols:\n",
        "    daily_feat[diag_cols] = daily_feat[diag_cols].fillna(0)\n",
        "\n",
        "# ---------- A1c prior & future via searchsorted (no merge_asof) ----------\n",
        "daily_feat = daily_feat.sort_values([\"Patient_ID\",\"index_date\"]).reset_index(drop=True)\n",
        "\n",
        "# Prepare outputs\n",
        "daily_feat[\"a1c_prior\"]  = np.nan\n",
        "daily_feat[\"a1c_future\"] = np.nan\n",
        "\n",
        "if len(a1c):\n",
        "    # groupwise arrays for fast lookup\n",
        "    g_df  = daily_feat.groupby(\"Patient_ID\", sort=False)\n",
        "    g_a1c = a1c.groupby(\"Patient_ID\", sort=False)\n",
        "\n",
        "    # Iterate over patients present in daily_feat\n",
        "    for pid, idx in g_df.groups.items():\n",
        "        # daily index dates for this patient (sorted because daily_feat is sorted)\n",
        "        idx_dates = daily_feat.loc[idx, \"index_date\"].to_numpy()\n",
        "\n",
        "        # if patient has A1c records\n",
        "        if pid in g_a1c.groups:\n",
        "            a_idx   = g_a1c.groups[pid]\n",
        "            a_dates = a1c.loc[a_idx, \"date\"].to_numpy()       # sorted\n",
        "            a_vals  = a1c.loc[a_idx, \"a1c\"].to_numpy()\n",
        "\n",
        "            # PRIOR: for each index_date, find last A1c date <= index_date\n",
        "            pos = np.searchsorted(a_dates, idx_dates, side=\"right\") - 1\n",
        "            valid = pos >= 0\n",
        "            prior_vals = np.full(len(idx_dates), np.nan, dtype=\"float64\")\n",
        "            prior_vals[valid] = a_vals[pos[valid]]\n",
        "            daily_feat.loc[idx, \"a1c_prior\"] = prior_vals\n",
        "\n",
        "            # FUTURE: next A1c within 90 days → find first A1c date >= index_date\n",
        "            pos_f = np.searchsorted(a_dates, idx_dates, side=\"left\")\n",
        "            future_vals = np.full(len(idx_dates), np.nan, dtype=\"float64\")\n",
        "            in_range = (pos_f < len(a_dates))\n",
        "            if in_range.any():\n",
        "                # check within 90 days\n",
        "                d_ok = (a_dates[pos_f[in_range]] - idx_dates[in_range]) <= np.timedelta64(90, \"D\")\n",
        "                future_vals[in_range] = np.where(d_ok, a_vals[pos_f[in_range]], np.nan)\n",
        "            daily_feat.loc[idx, \"a1c_future\"] = future_vals\n",
        "        else:\n",
        "            # no A1c for this patient → remain NaN\n",
        "            pass\n",
        "\n",
        "# ---------- Future 90d hyperglycemia burden ----------\n",
        "def future_mean_next_n_days(group, col, n=90):\n",
        "    s = group[col].astype(float)\n",
        "    s_rev = s.iloc[::-1].rolling(window=n, min_periods=7).mean().iloc[::-1]\n",
        "    return s_rev.shift(-1)\n",
        "\n",
        "daily_feat = daily_feat.sort_values([\"Patient_ID\",\"date\"]).reset_index(drop=True)\n",
        "daily_feat[\"future90_frac_gt_180_mean\"] = (\n",
        "    daily_feat.groupby(\"Patient_ID\", group_keys=False)\n",
        "              .apply(lambda g: future_mean_next_n_days(g, \"frac_gt_180\", 90))\n",
        ")\n",
        "\n",
        "# ---------- Label ----------\n",
        "daily_feat[\"label_90d\"] = (\n",
        "    (daily_feat[\"future90_frac_gt_180_mean\"] >= FUTURE_90D_HYPER_MEAN_THRESHOLD) |\n",
        "    ((daily_feat[\"a1c_future\"] - daily_feat[\"a1c_prior\"]) >= A1C_RISE_THRESHOLD)\n",
        ").astype(int)\n",
        "\n",
        "# ---------- Keep valid rows ----------\n",
        "if \"mean_glu_mean_30\" not in daily_feat.columns:\n",
        "    raise RuntimeError(\"mean_glu_mean_30 not found; rolling features may have failed.\")\n",
        "\n",
        "mask_hist   = daily_feat[\"mean_glu_mean_30\"].notna()\n",
        "mask_future = daily_feat[\"future90_frac_gt_180_mean\"].notna()\n",
        "dataset = daily_feat[mask_hist & mask_future].copy()\n",
        "\n",
        "# ---------- Feature set ----------\n",
        "feature_cols = [\n",
        "    c for c in dataset.columns\n",
        "    if any(tok in c for tok in [\"_mean_\", \"_std_\", \"_slope_\", \"coverage\", \"n_reads\"])\n",
        "    and \"future90\" not in c\n",
        "]\n",
        "feature_cols += [\"Birth_year\"]\n",
        "if \"Sex\" in dataset.columns:\n",
        "    dataset[\"Sex_enc\"] = dataset[\"Sex\"].astype(\"category\").cat.codes\n",
        "    feature_cols += [\"Sex_enc\"]\n",
        "feature_cols += [c for c in dataset.columns if c.startswith(\"diag_\")]\n",
        "\n",
        "keep_cols = [\"Patient_ID\",\"date\",\"index_date\",\"label_90d\"] + feature_cols + [\"a1c_prior\",\"a1c_future\"]\n",
        "dataset = dataset[keep_cols].copy()\n",
        "dataset = dataset.dropna(subset=feature_cols, how=\"all\")\n",
        "\n",
        "print(\"Modeling dataset shape:\", dataset.shape)\n",
        "display(dataset.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC: 0.940837009718153 AUPRC: 0.9508171157147218 Brier: 0.10052916014623206 CM@0.5: [[13704, 1892], [3191, 15887]]\n",
            "Artifacts saved in: C:\\Users\\91954\\Desktop\\22mia1062\\welldoc\\outputs\n"
          ]
        }
      ],
      "source": [
        "# --- Train, calibrate, evaluate ---\n",
        "# Patient-level split to avoid leakage\n",
        "patients = dataset[\"Patient_ID\"].unique()\n",
        "rng = np.random.default_rng(42)\n",
        "test_pat = set(rng.choice(patients, size=int(0.2*len(patients)), replace=False))\n",
        "\n",
        "train_df = dataset[~dataset[\"Patient_ID\"].isin(test_pat)].copy()\n",
        "test_df  = dataset[ dataset[\"Patient_ID\"].isin(test_pat)].copy()\n",
        "\n",
        "y_tr = train_df[\"label_90d\"].values\n",
        "y_te = test_df[\"label_90d\"].values\n",
        "\n",
        "X_tr = train_df[feature_cols]\n",
        "X_te = test_df[feature_cols]\n",
        "\n",
        "# simple preprocessing (impute medians)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "pre = ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), feature_cols)], remainder=\"drop\")\n",
        "\n",
        "if HAS_XGB:\n",
        "    clf = XGBClassifier(\n",
        "        n_estimators=400, max_depth=4, learning_rate=0.06,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        tree_method=\"hist\", random_state=7, eval_metric=\"logloss\",\n",
        "        scale_pos_weight=(y_tr==0).sum()/(y_tr==1).sum() if (y_tr==1).sum()>0 else 1.0\n",
        "    )\n",
        "else:\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "    clf = HistGradientBoostingClassifier(max_depth=3, learning_rate=0.06, max_iter=400, random_state=7)\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "pipe.fit(X_tr, y_tr)\n",
        "\n",
        "# Calibrate\n",
        "cal = CalibratedClassifierCV(pipe, cv=3, method=\"isotonic\")\n",
        "cal.fit(X_tr, y_tr)\n",
        "\n",
        "# Evaluate\n",
        "proba = cal.predict_proba(X_te)[:,1]\n",
        "auroc = roc_auc_score(y_te, proba) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "auprc = average_precision_score(y_te, proba) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "brier = brier_score_loss(y_te, proba)\n",
        "\n",
        "thr = 0.5\n",
        "pred = (proba >= thr).astype(int)\n",
        "cm = confusion_matrix(y_te, pred).tolist()\n",
        "\n",
        "print(\"AUROC:\", auroc, \"AUPRC:\", auprc, \"Brier:\", brier, \"CM@0.5:\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "import joblib\n",
        "joblib.dump(cal, OUT / \"model_calibrated.joblib\")\n",
        "with open(OUT / \"metrics.json\",\"w\") as f:\n",
        "    json.dump({\"AUROC\":auroc, \"AUPRC\":auprc, \"Brier\":brier, \"ConfusionMatrix@0.5\":cm}, f, indent=2)\n",
        "with open(OUT / \"feature_meta.json\",\"w\") as f:\n",
        "    json.dump({\"feature_cols\": feature_cols}, f, indent=2)\n",
        "\n",
        "# Save dataset for app/demo\n",
        "dataset.to_parquet(OUT / \"feature_dataset.parquet\", index=False)\n",
        "\n",
        "# Plots (matplotlib only)\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "fig, ax = plt.subplots()\n",
        "RocCurveDisplay.from_predictions(y_te, proba, ax=ax); plt.savefig(PLOTS / \"roc_curve.png\"); plt.close()\n",
        "fig, ax = plt.subplots()\n",
        "PrecisionRecallDisplay.from_predictions(y_te, proba, ax=ax); plt.savefig(PLOTS / \"pr_curve.png\"); plt.close()\n",
        "\n",
        "# Calibration plot\n",
        "from sklearn.calibration import calibration_curve\n",
        "prob_true, prob_pred = calibration_curve(y_te, proba, n_bins=10, strategy='quantile')\n",
        "plt.plot(prob_pred, prob_true, marker='o'); plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.title(\"Calibration\")\n",
        "plt.savefig(PLOTS / \"calibration.png\"); plt.close()\n",
        "\n",
        "print(\"Artifacts saved in:\", OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 301it [01:16,  3.40it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved global SHAP beeswarm -> C:\\Users\\91954\\Desktop\\22mia1062\\welldoc\\outputs\\plots\\shap_global.png\n",
            "Saved global SHAP bar -> C:\\Users\\91954\\Desktop\\22mia1062\\welldoc\\outputs\\plots\\shap_bar.png\n"
          ]
        }
      ],
      "source": [
        "# --- Explainability (global + robust multi-output handling) ---\n",
        "HAS_SHAP = True\n",
        "try:\n",
        "    import shap, matplotlib.pyplot as plt\n",
        "except Exception as e:\n",
        "    HAS_SHAP = False\n",
        "    print(\"SHAP not available:\", e)\n",
        "\n",
        "if HAS_SHAP:\n",
        "    # sample for speed\n",
        "    sample = train_df.sample(n=min(300, len(train_df)), random_state=7)\n",
        "    Xs = sample[feature_cols]\n",
        "\n",
        "    # Try fast tree path; else fall back to generic Explainer\n",
        "    exp = None\n",
        "    try:\n",
        "        # Try to access underlying tree model inside calibrated pipeline\n",
        "        base_est = cal.estimators_[0].named_steps[\"clf\"] if hasattr(cal, \"estimators_\") else None\n",
        "        preproc  = cal.estimators_[0].named_steps[\"pre\"] if hasattr(cal, \"estimators_\") else None\n",
        "        if base_est is None or preproc is None:\n",
        "            raise RuntimeError(\"No inner estimator/preprocessor available; using generic SHAP.\")\n",
        "\n",
        "        # transform X with the pipeline's preprocessor\n",
        "        Xs_t = preproc.transform(Xs)\n",
        "\n",
        "        # TreeExplainer on the tree model (XGB/HGB/etc.)\n",
        "        tree_explainer = shap.TreeExplainer(base_est)\n",
        "        sv = tree_explainer.shap_values(Xs_t)\n",
        "\n",
        "        # Normalize outputs to a SHAP Explanation with original feature names\n",
        "        if isinstance(sv, list):           # some versions return [class0, class1]\n",
        "            sv_class1 = sv[1]              # choose positive class\n",
        "        else:\n",
        "            sv_class1 = sv                 # already 2-D\n",
        "        exp = shap.Explanation(\n",
        "            values=sv_class1,\n",
        "            base_values=(np.mean(tree_explainer.expected_value)\n",
        "                         if np.ndim(tree_explainer.expected_value)>0\n",
        "                         else tree_explainer.expected_value),\n",
        "            data=Xs,\n",
        "            feature_names=feature_cols,\n",
        "        )\n",
        "    except Exception:\n",
        "        # Generic Explainer on proba; may return multi-output Explanation\n",
        "        exp = shap.Explainer(cal.predict_proba, Xs, feature_names=feature_cols)(Xs)\n",
        "\n",
        "        # >>>>>>> KEY FIX: collapse multi-output to class 1 for plotting\n",
        "        try:\n",
        "            if hasattr(exp, \"values\") and getattr(exp.values, \"ndim\", 2) == 3:\n",
        "                # shape: (n_samples, n_classes, n_features) -> pick class 1\n",
        "                exp = exp[:, :, 1]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Now exp should be 2-D (n_samples, n_features)\n",
        "    shap.plots.beeswarm(exp, max_display=20, show=False)\n",
        "    plt.tight_layout(); plt.savefig(PLOTS / \"shap_global.png\", dpi=200); plt.close()\n",
        "    print(\"Saved global SHAP beeswarm ->\", PLOTS / \"shap_global.png\")\n",
        "\n",
        "    shap.plots.bar(exp, max_display=20, show=False)\n",
        "    plt.tight_layout(); plt.savefig(PLOTS / \"shap_bar.png\", dpi=200); plt.close()\n",
        "    print(\"Saved global SHAP bar ->\", PLOTS / \"shap_bar.png\")\n",
        "else:\n",
        "    print(\"SHAP not installed; skipping.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead1f04e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local plots saved for LIB193263\n"
          ]
        }
      ],
      "source": [
        "# Pick a concrete patient + their latest index_date\n",
        "target_pid = dataset[\"Patient_ID\"].iloc[0]  # replace with any ID you want\n",
        "\n",
        "row = (dataset[dataset[\"Patient_ID\"] == target_pid]\n",
        "       .sort_values(\"index_date\")\n",
        "       .tail(1))\n",
        "X1 = row[feature_cols]\n",
        "\n",
        "local_exp = shap.Explainer(cal.predict_proba, X1, feature_names=feature_cols)(X1)\n",
        "# If multi-output, reduce to class 1\n",
        "if hasattr(local_exp, \"values\") and getattr(local_exp.values, \"ndim\", 2) == 3:\n",
        "    local_exp = local_exp[:, :, 1]\n",
        "\n",
        "# Waterfall = ranked top drivers for THIS prediction\n",
        "shap.plots.waterfall(local_exp[0], max_display=15, show=False)\n",
        "plt.tight_layout(); plt.savefig(PLOTS / f\"shap_local_waterfall_{target_pid}.png\", dpi=200); plt.close()\n",
        "\n",
        "# Force = compact push/pull view\n",
        "shap.plots.force(local_exp[0], matplotlib=True, show=False)\n",
        "plt.tight_layout(); plt.savefig(PLOTS / f\"shap_local_force_{target_pid}.png\", dpi=200); plt.close()\n",
        "\n",
        "print(\"Local plots saved for\", target_pid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37abb19",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 301it [01:16,  3.43it/s]                         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved top_global_drivers.csv and top_local_drivers.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Global: mean |SHAP|\n",
        "exp_for_table = shap.Explainer(cal.predict_proba, train_df[feature_cols], feature_names=feature_cols)(train_df[feature_cols].sample(300, random_state=7))\n",
        "if hasattr(exp_for_table.values, \"ndim\") and exp_for_table.values.ndim == 3:\n",
        "    exp_for_table = exp_for_table[:, :, 1]\n",
        "\n",
        "global_top = (pd.DataFrame({\n",
        "    \"feature\": feature_cols,\n",
        "    \"mean_abs_shap\": np.abs(exp_for_table.values).mean(axis=0)\n",
        "}).sort_values(\"mean_abs_shap\", ascending=False).head(20))\n",
        "global_top.to_csv(OUT / \"top_global_drivers.csv\", index=False)\n",
        "\n",
        "# Local: feature contributions for the chosen patient\n",
        "local_tbl = (pd.DataFrame({\n",
        "    \"feature\": feature_cols,\n",
        "    \"shap_value\": local_exp.values[0],\n",
        "    \"abs_shap\": np.abs(local_exp.values[0])\n",
        "}).sort_values(\"abs_shap\", ascending=False).head(20))\n",
        "local_tbl.to_csv(OUT / \"top_local_drivers.csv\", index=False)\n",
        "\n",
        "print(\"Saved top_global_drivers.csv and top_local_drivers.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68526c1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   threshold  precision    recall        f1\n",
            "0        0.1   0.688506  0.990932  0.812489\n",
            "1        0.2   0.752639  0.979086  0.851057\n",
            "2        0.3   0.810739  0.957648  0.878091\n",
            "3        0.4   0.856770  0.911783  0.883421\n",
            "4        0.5   0.893582  0.832739  0.862089\n",
            "5        0.6   0.921929  0.754534  0.829874\n",
            "6        0.7   0.939782  0.683877  0.791663\n",
            "7        0.8   0.966623  0.584443  0.728449\n",
            "8        0.9   0.990433  0.439564  0.608895\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "thr_grid = np.round(np.linspace(0.1, 0.9, 9), 2)\n",
        "rows = []\n",
        "for t in thr_grid:\n",
        "    pred = (proba >= t).astype(int)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_te, pred, average=\"binary\", zero_division=0)\n",
        "    rows.append({\"threshold\": t, \"precision\": p, \"recall\": r, \"f1\": f1})\n",
        "thr_df = pd.DataFrame(rows)\n",
        "print(thr_df)\n",
        "thr_df.to_csv(OUT / \"threshold_sweep.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e07597b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUROC: 0.9753551597520157\n",
            "Saved: outputs\\plots\\calibration_quantile.png and outputs\\plots\\decision_curve.png\n"
          ]
        }
      ],
      "source": [
        "# Recompute test split, predict proba, and save calibration & decision curve plots\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import joblib\n",
        "\n",
        "BASE = Path(\".\")\n",
        "OUT = BASE / \"outputs\"; PLOTS = OUT / \"plots\"\n",
        "DATASET_PATH = OUT / \"feature_dataset.parquet\"\n",
        "MODEL_PATH   = OUT / \"model_calibrated.joblib\"\n",
        "\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Load dataset + model\n",
        "df = pd.read_parquet(DATASET_PATH)\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "# 2) Patient-level split (80/20 by Patient_ID, fixed seed)\n",
        "rng = np.random.RandomState(42)\n",
        "pids = np.array(sorted(df[\"Patient_ID\"].unique()))\n",
        "rng.shuffle(pids)\n",
        "cut = int(0.8 * len(pids))\n",
        "train_pids, test_pids = set(pids[:cut]), set(pids[cut:])\n",
        "\n",
        "test_df  = df[df[\"Patient_ID\"].isin(test_pids)].copy()\n",
        "feature_cols = [c for c in df.columns if c not in\n",
        "                [\"Patient_ID\",\"date\",\"index_date\",\"label_90d\",\"a1c_prior\",\"a1c_future\"]]\n",
        "X_te = test_df[feature_cols]\n",
        "y_te = test_df[\"label_90d\"].astype(int).values\n",
        "\n",
        "# 3) Predict probabilities (fallback to predict if necessary)\n",
        "try:\n",
        "    proba = model.predict_proba(X_te)[:, 1]\n",
        "except Exception:\n",
        "    proba = model.predict(X_te).astype(float).clip(0, 1)\n",
        "\n",
        "print(\"Test AUROC:\", roc_auc_score(y_te, proba))\n",
        "\n",
        "# 4) Calibration (quantile-binned)\n",
        "prob_true, prob_pred = calibration_curve(y_te, proba, n_bins=10, strategy=\"quantile\")\n",
        "pd.DataFrame(\n",
        "    {\"bin\": np.arange(1, len(prob_true)+1),\n",
        "     \"mean_pred\": prob_pred,\n",
        "     \"frac_positive\": prob_true}\n",
        ").to_csv(OUT / \"calibration_quantile.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(5.5, 5))\n",
        "plt.plot([0,1], [0,1], \"--\", lw=1, label=\"Perfectly calibrated\")\n",
        "plt.plot(prob_pred, prob_true, marker=\"o\", lw=2, label=\"Model\")\n",
        "plt.xlabel(\"Predicted probability\")\n",
        "plt.ylabel(\"Observed fraction positive\")\n",
        "plt.title(\"Calibration (quantile-binned)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS / \"calibration_quantile.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# 5) Decision Curve Analysis (model vs treat-all vs treat-none)\n",
        "ths = np.linspace(0.05, 0.95, 19)\n",
        "nb_model, nb_all, nb_none = [], [], []\n",
        "N = len(y_te)\n",
        "\n",
        "for t in ths:\n",
        "    pred = (proba >= t).astype(int)\n",
        "    TP = np.sum((pred == 1) & (y_te == 1))\n",
        "    FP = np.sum((pred == 1) & (y_te == 0))\n",
        "    w  = t / (1 - t)  # threshold -> harm:benefit ratio\n",
        "    nb_model.append((TP / N) - (FP / N) * w)\n",
        "\n",
        "    TP_all = np.sum(y_te == 1)\n",
        "    FP_all = np.sum(y_te == 0)\n",
        "    nb_all.append((TP_all / N) - (FP_all / N) * w)\n",
        "\n",
        "    nb_none.append(0.0)\n",
        "\n",
        "nb_df = pd.DataFrame({\"threshold\": ths, \"Model\": nb_model, \"Treat all\": nb_all, \"Treat none\": nb_none})\n",
        "nb_df.to_csv(OUT / \"decision_curve.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(6.5, 4.5))\n",
        "plt.plot(nb_df[\"threshold\"], nb_df[\"Model\"], label=\"Model\", lw=2)\n",
        "plt.plot(nb_df[\"threshold\"], nb_df[\"Treat all\"], label=\"Treat all\", lw=1)\n",
        "plt.plot(nb_df[\"threshold\"], nb_df[\"Treat none\"], label=\"Treat none\", lw=1)\n",
        "plt.axhline(0, color=\"k\", lw=0.8)\n",
        "plt.xlabel(\"Risk threshold\")\n",
        "plt.ylabel(\"Net benefit\")\n",
        "plt.title(\"Decision Curve Analysis\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS / \"decision_curve.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved:\", PLOTS / \"calibration_quantile.png\", \"and\", PLOTS / \"decision_curve.png\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
